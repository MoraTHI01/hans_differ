Embeddings on EMBEDDINGS_HANS/embeddings_remote_api.sh
API service for LLM and VLLM are in SERAPHIM: 10.16.246.2:8093; RLLM is on SERAPHIM TOO
LLM and VLLM through seraphim:

Port: 8093
Model:  mistralai/Mistral-Small-3.1-24B-Instruct-2503
Max_Model_length: 40000

Deployed via SERAPHIM:

--------------------------------------------------------------------------------------------


#!/bin/bash
#SBATCH --job-name=vllm_service_model_pHAnS_Mistral_p40000
#SBATCH --output=/home/aimotion_api/SERAPHIM/scripts/vllm_service_model_pHAnS_Mistral_p40000_%j.out
#SBATCH --error=/home/aimotion_api/SERAPHIM/scripts/vllm_service_model_pHAnS_Mistral_p40000_%j.err
#SBATCH --time=99:00:00
#SBATCH --gres=gpu:1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=4
#SBATCH --mem=32G
#SBATCH --mail-type=NONE
echo "Current ulimit -n (soft): $(ulimit -Sn)"
echo "Current ulimit -n (hard): $(ulimit -Hn)"
ulimit -n 10240 
if [ $? -eq 0 ]; then echo "Successfully set ulimit -n to $(ulimit -Sn)"; else echo "WARN: Failed to set ulimit -n. Current: $(ulimit -Sn). Check hard limits if issues persist."; fi
echo "=================================================================="
echo "✝ SERAPHIM vLLM Deployment Job - SLURM PREP ✝"
echo "Job Start Time: $(date)"
echo "Job ID: $SLURM_JOB_ID running on Node: $(hostname -f) (Short: $(hostname -s))"
echo "Slurm Output File: /home/aimotion_api/SERAPHIM/scripts/vllm_service_model_pHAnS_Mistral_p40000_$SLURM_JOB_ID.out"
echo "Slurm Error File: /home/aimotion_api/SERAPHIM/scripts/vllm_service_model_pHAnS_Mistral_p40000_$SLURM_JOB_ID.err"
echo "Model Identifier: mistralai/Mistral-Small-3.1-24B-Instruct-2503"
echo "Target Service Port: 40000"
echo "Conda Env: seraphim_vllm_env"
echo "Max Model Length Requested: 16384"
echo "vLLM service will run in the FOREGROUND of this Slurm job."
echo "=================================================================="
CONDA_BASE_PATH_SLURM="$(conda info --base)"
if [ -z "$CONDA_BASE_PATH_SLURM" ]; then echo "ERROR: Conda base path empty."; exit 1; fi
CONDA_SH_PATH="$CONDA_BASE_PATH_SLURM/etc/profile.d/conda.sh"
if [ -f "$CONDA_SH_PATH" ]; then . "$CONDA_SH_PATH"; else echo "WARN: conda.sh not found."; fi
conda activate "seraphim_vllm_env"
if [[ "$CONDA_PREFIX" != *"seraphim_vllm_env"* ]]; then echo "ERROR: Failed to activate conda. Prefix: $CONDA_PREFIX"; exit 1; fi
echo "Conda env 'seraphim_vllm_env' activated. Path: $CONDA_PREFIX";
HF_TOKEN_VALUE=""
if [ -n "$HF_TOKEN_VALUE" ]; then export HF_TOKEN="$HF_TOKEN_VALUE"; echo "HF_TOKEN set."; else echo "HF_TOKEN not provided."; fi
export VLLM_CONFIGURE_LOGGING="0"
export VLLM_NO_USAGE_STATS="True"
export VLLM_DO_NOT_TRACK="True"
export VLLM_ALLOW_LONG_MAX_MODEL_LEN="1"
echo "VLLM_ALLOW_LONG_MAX_MODEL_LEN set to 1."
echo -e "\nStarting vLLM API Server in FOREGROUND..."
echo "Command: vllm serve "mistralai/Mistral-Small-3.1-24B-Instruct-2503" \
    --host "0.0.0.0" \
    --port 40000 \
    --trust-remote-code \
    --max-model-len 16384"
echo "vLLM logs will be in Slurm output/error files."
echo "--- vLLM Service Starting (Output will follow) ---"
vllm serve "mistralai/Mistral-Small-3.1-24B-Instruct-2503" \
    --host "0.0.0.0" \
    --port 40000 \
    --trust-remote-code \
    --max-model-len 16384
VLLM_EXIT_CODE=$?
echo "--- vLLM Service Ended (Exit Code: $VLLM_EXIT_CODE) ---"
echo "=================================================================="
echo "✝ SERAPHIM vLLM Job - FINAL STATUS ✝"
if [ $VLLM_EXIT_CODE -eq 0 ]; then echo "vLLM exited cleanly or was terminated."; else echo "ERROR: vLLM exited with code: $VLLM_EXIT_CODE."; fi
echo "Slurm job $SLURM_JOB_ID finished."
echo "=================================================================="



---------------------------------------------------------

RLLM: 
port 8090


